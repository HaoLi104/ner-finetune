#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯¹å®Œæ•´çš„project-1.converted.jsonè¿›è¡Œé¢„è®¡ç®—ï¼Œæ”¯æŒå¤šæ–‡ä»¶åˆå¹¶å’ŒæŠ¥å‘Šç±»å‹æ›¿æ¢
"""

import os
import sys
import json
from pathlib import Path
from typing import List, Dict, Any, Optional


sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from pre_struct.ebqa.da_core.dataset import EnhancedQADataset



def _save_json(data, path):
    """ä¿å­˜ä¸ºJSONæ ¼å¼"""
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def _save_jsonl(samples, path):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    with open(path, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')


def merge_and_replace_report_types(
    base_file: str,
    replacement_files: Dict[str, str],
    output_file: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """åˆå¹¶å¤šä¸ªæ•°æ®æ–‡ä»¶ï¼Œå¹¶æŒ‰æŠ¥å‘Šç±»å‹æ›¿æ¢
    
    Args:
        base_file: åŸºç¡€æ–‡ä»¶è·¯å¾„ï¼ˆä¸»è®­ç»ƒæ–‡ä»¶ï¼‰
        replacement_files: {æŠ¥å‘Šç±»å‹: æ–‡ä»¶è·¯å¾„} çš„æ˜ å°„
            ä¾‹å¦‚: {"å…¥é™¢è®°å½•": "data/ruyuanjilu/ruyuan-2025-10-16.converted.json"}
        output_file: åˆå¹¶åçš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        åˆå¹¶åçš„è®°å½•åˆ—è¡¨
    
    Example:
        merged = merge_and_replace_report_types(
            base_file="data/project-1.converted.json",
            replacement_files={
                "å…¥é™¢è®°å½•": "data/ruyuanjilu/ruyuan-2025-10-16.converted.json",
                "å‡ºé™¢è®°å½•": "data/other/chuyuan.converted.json",
            },
            output_file="data/merged.converted.json"
        )
    """
    # è¯»å–åŸºç¡€æ–‡ä»¶
    print(f"è¯»å–åŸºç¡€æ–‡ä»¶: {base_file}")
    with open(base_file, 'r', encoding='utf-8') as f:
        base_data = json.load(f)
    
    base_by_type = {}
    for rec in base_data:
        report_type = rec.get("report_title", "")
        if report_type not in base_by_type:
            base_by_type[report_type] = []
        base_by_type[report_type].append(rec)
    
    print(f"  åŸºç¡€æ–‡ä»¶åŒ…å« {len(base_data)} æ¡è®°å½•")
    print(f"  æŠ¥å‘Šç±»å‹åˆ†å¸ƒ:")
    for rt, recs in sorted(base_by_type.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"    {rt}: {len(recs)} æ¡")
    
    # è¯»å–æ›¿æ¢æ–‡ä»¶
    replacement_data = {}
    for report_type, file_path in replacement_files.items():
        print(f"\nè¯»å–æ›¿æ¢æ–‡ä»¶: {file_path}")
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # åªæå–æŒ‡å®šæŠ¥å‘Šç±»å‹çš„è®°å½•
        filtered = [rec for rec in data if rec.get("report_title", "") == report_type]
        replacement_data[report_type] = filtered
        
        print(f"  æ€»è®°å½•: {len(data)}")
        print(f"  {report_type}: {len(filtered)} æ¡")
    
    # åˆå¹¶ï¼šç”¨replacementæ›¿æ¢baseä¸­çš„ç›¸åº”ç±»å‹
    print(f"\nåˆå¹¶ç­–ç•¥:")
    merged_by_type = {}
    
    for report_type, recs in base_by_type.items():
        if report_type in replacement_data:
            # æ›¿æ¢
            merged_by_type[report_type] = replacement_data[report_type]
            print(f"  {report_type}: ä½¿ç”¨æ›¿æ¢æ–‡ä»¶ ({len(base_by_type[report_type])} -> {len(replacement_data[report_type])})")
        else:
            # ä¿ç•™baseä¸­çš„
            merged_by_type[report_type] = recs
            print(f"  {report_type}: ä¿ç•™åŸºç¡€æ–‡ä»¶ ({len(recs)})")
    
    # æ·»åŠ replacementä¸­æœ‰ä½†baseä¸­æ²¡æœ‰çš„ç±»å‹
    for report_type, recs in replacement_data.items():
        if report_type not in merged_by_type:
            merged_by_type[report_type] = recs
            print(f"  {report_type}: æ–°å¢ç±»å‹ ({len(recs)})")
    
    # åˆå¹¶ä¸ºåˆ—è¡¨
    merged_records = []
    for report_type in sorted(merged_by_type.keys()):
        merged_records.extend(merged_by_type[report_type])
    
    print(f"\nåˆå¹¶ç»“æœ:")
    print(f"  æ€»è®°å½•æ•°: {len(merged_records)}")
    print(f"  æŠ¥å‘Šç±»å‹åˆ†å¸ƒ:")
    for rt in sorted(merged_by_type.keys(), key=lambda x: len(merged_by_type[x]), reverse=True):
        print(f"    {rt}: {len(merged_by_type[rt])} æ¡")
    
    # ä¿å­˜åˆ°æ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if output_file:
        print(f"\nä¿å­˜åˆå¹¶ç»“æœåˆ°: {output_file}")
        _save_json(merged_records, output_file)
    
    return merged_records

def main():
    # ===== é…ç½®åŒºåŸŸ =====
    # æ˜¯å¦å¯ç”¨åˆå¹¶å’Œæ›¿æ¢åŠŸèƒ½
    USE_MERGE = True
    
    if USE_MERGE:
        # åˆå¹¶æ¨¡å¼ï¼šç”¨Aæ–‡ä»¶ä¸­çš„æŠ¥å‘Šç±»å‹æ›¿æ¢Bæ–‡ä»¶ä¸­çš„æŠ¥å‘Šç±»å‹
        BASE_FILE = "data/project-1.converted.json"
        REPLACEMENT_FILES = {
            "å…¥é™¢è®°å½•": "data/ruyuanjilu/ruyuan-2025-10-16.converted.json",
            # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æŠ¥å‘Šç±»å‹çš„æ›¿æ¢
            # "å‡ºé™¢è®°å½•": "data/other/chuyuan.converted.json",
        }
        MERGED_FILE = "data/merged.converted.json"
        INPUT_JSON = MERGED_FILE
    else:
        # ç›´æ¥æ¨¡å¼ï¼šä¸åˆå¹¶ï¼Œç›´æ¥é¢„è®¡ç®—
        INPUT_JSON = "data/project-1.converted.json"
    
    OUTPUT_JSONL = INPUT_JSON.replace(".json", ".jsonl")
    
    print("=" * 80)
    print("ğŸš€ é¢„è®¡ç®—å®Œæ•´æ•°æ®é›†")
    print("=" * 80)
    
    # å¦‚æœå¯ç”¨åˆå¹¶ï¼Œå…ˆæ‰§è¡Œåˆå¹¶
    if USE_MERGE:
        print("\nğŸ“ æ­¥éª¤1: åˆå¹¶å’Œæ›¿æ¢æŠ¥å‘Šç±»å‹")
        print("=" * 80)
        merged_data = merge_and_replace_report_types(
            base_file=BASE_FILE,
            replacement_files=REPLACEMENT_FILES,
            output_file=MERGED_FILE,
        )
        print(f"\nâœ“ åˆå¹¶å®Œæˆï¼Œä¿å­˜åˆ°: {MERGED_FILE}")
        print("=" * 80)
    
    print(f"\nğŸ“Š æ­¥éª¤2: é¢„è®¡ç®—æ ·æœ¬")
    print("=" * 80)
    print(f"è¾“å…¥: {INPUT_JSON}")
    print(f"è¾“å‡º: {OUTPUT_JSONL}")
    print()
    
    # ä»é…ç½®è·å–tokenizer
    with open("pre_struct/ebqa/ebqa_config.json", 'r') as f:
        cfg = json.load(f)
        tokenizer_path = cfg.get("tokenizer_name_or_path")
    
    print(f"âœ… Tokenizer: {tokenizer_path}")
    print()
    
    # æ„å»ºæ•°æ®é›†
    print("â³ æ­£åœ¨æ„å»ºæ•°æ®é›†ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰...")
    ds = EnhancedQADataset(
        data_path=INPUT_JSON,
        tokenizer_name=tokenizer_path,
        max_seq_len=512,
        max_tokens_ctx=500,
        max_answer_len=512,
        use_question_templates=True,
        keep_debug_fields=True,
        report_struct_path="keys/keys_merged.json",
        only_title_keys=True,
        inference_mode=False,
        dynamic_answer_length=True,
        negative_downsample=0.2,  # ä½¿ç”¨æœ€ä¼˜é…ç½®
        chunk_mode="budget",
        seed=42,
        autobuild=True,
        show_progress=True,
        use_concurrent_build=False,  # ä¸²è¡Œï¼Œé¿å…å¡æ­»
        max_workers=None,
    )
    
    print()
    print("â³ ä¿å­˜é¢„è®¡ç®—æ ·æœ¬...")
    _save_jsonl(ds.samples, OUTPUT_JSONL)
    
    print()
    print("=" * 70)
    print("âœ… é¢„è®¡ç®—å®Œæˆ!")
    print("=" * 70)
    print(f"è¾“å…¥è®°å½•: {len(ds.records)}")
    print(f"è¾“å‡ºæ ·æœ¬: {len(ds.samples)}")
    
    # ç»Ÿè®¡
    pos_count = sum(1 for s in ds.samples if s.get('start_positions', 0) != 0)
    neg_count = len(ds.samples) - pos_count
    
    print(f"æ­£æ ·æœ¬: {pos_count} ({pos_count/len(ds.samples)*100:.1f}%)")
    print(f"è´Ÿæ ·æœ¬: {neg_count} ({neg_count/len(ds.samples)*100:.1f}%)")
    print()

if __name__ == "__main__":
    main()

