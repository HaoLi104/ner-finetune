#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é¢„è®¡ç®— alias/title QA æ ·æœ¬ï¼ˆä½¿ç”¨ merged.converted.json çš„ alias å­—æ®µï¼‰
"""

import os
import sys
import json
from pathlib import Path

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from pre_struct.ebqa_title.da_core.dataset import EnhancedQADataset, QACollator

# ==================== é…ç½® ====================
ENABLE_QUICK_TEST = False  # æ”¹ä¸ºFalseåšå®Œæ•´é¢„è®¡ç®—
QUICK_TEST_SIZE = 500


def _save_jsonl(samples, path):
    """ä¿å­˜ä¸ºJSONLæ ¼å¼"""
    with open(path, 'w', encoding='utf-8') as f:
        for sample in samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')


def main():
    # ===== é…ç½®åŒºåŸŸ =====
    INPUT_JSON = "data/merged.converted.json"
    OUTPUT_JSONL = INPUT_JSON.replace(".json", ".alias_title.jsonl")
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰Næ¡è®°å½•
    if ENABLE_QUICK_TEST:
        print(f"âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰{QUICK_TEST_SIZE}æ¡è®°å½•")
        with open(INPUT_JSON, 'r', encoding='utf-8') as f:
            all_data = json.load(f)
        test_data = all_data[:QUICK_TEST_SIZE]
        INPUT_JSON = "data/.tmp_quick_test.json"
        with open(INPUT_JSON, 'w', encoding='utf-8') as f:
            json.dump(test_data, f, ensure_ascii=False)
        OUTPUT_JSONL = INPUT_JSON.replace(".json", ".alias_title.jsonl")
        print(f"âœ“ ä¸´æ—¶æ–‡ä»¶: {INPUT_JSON}")
    
    print("=" * 80)
    print("ğŸš€ é¢„è®¡ç®— Alias/Title QA æ•°æ®é›†")
    print("=" * 80)
    print(f"è¾“å…¥: {INPUT_JSON}")
    print(f"è¾“å‡º: {OUTPUT_JSONL}")
    print()
    
    # åªä» ebqa_title çš„ merged_config.json è¯»å–å‚æ•°ï¼ˆä¸å†ä½¿ç”¨ä»»ä½•é»˜è®¤å€¼ï¼‰
    config_path = "pre_struct/ebqa_title/merged_config.json"
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"é…ç½®æœªæ‰¾åˆ°: {config_path}")
    with open(config_path, 'r', encoding='utf-8') as f:
        cfg = json.load(f)
    if not isinstance(cfg, dict):
        raise ValueError("é…ç½®æ–‡ä»¶å¿…é¡»æ˜¯ JSON å¯¹è±¡")

    tokenizer_path = cfg.get("tokenizer_name_or_path")
    qtpl = ((cfg.get("train", {}) or {}).get("question_template") or "").strip()
    if not tokenizer_path:
        raise KeyError("tokenizer_name_or_path ç¼ºå¤±ï¼Œè¯·åœ¨ merged_config.json ä¸­é…ç½®")
    if not qtpl:
        raise KeyError("train.question_template ç¼ºå¤±ï¼Œè¯·åœ¨ merged_config.json ä¸­é…ç½®")

    print(f"âœ… Tokenizer: {tokenizer_path}")
    print(f"âœ… é—®é¢˜æ¨¡æ¿: {qtpl}")
    print()
    
    # æ„å»ºæ•°æ®é›†
    print("â³ æ­£åœ¨æ„å»ºæ•°æ®é›†...")
    ds = EnhancedQADataset(
        data_path=INPUT_JSON,
        tokenizer_name=tokenizer_path,
        max_seq_len=512,
        max_tokens_ctx=500,
        max_answer_len=512,
        use_question_templates=True,
        keep_debug_fields=True,
        report_struct_path="keys/keys_merged.json",
        only_title_keys=True,
        inference_mode=False,
        dynamic_answer_length=True,
        negative_downsample=1.0,  # ä¿ç•™æ‰€æœ‰è´Ÿæ ·æœ¬ï¼ˆå·²é€šè¿‡å­—æ®µåˆ†é…æ§åˆ¶æ•°é‡ï¼‰
        chunk_mode="budget",
        seed=42,
        autobuild=True,
        show_progress=True,
        # Alias/Title ç‰¹æœ‰å‚æ•°
        alias_field="alias",
        question_template=qtpl,
        use_concurrent_build=False,   # æ”¹å›ä¸²è¡Œæ¨¡å¼
        max_workers=None              # å•çº¿ç¨‹
    )
    
    print()
    print("â³ ä¿å­˜é¢„è®¡ç®—æ ·æœ¬...")
    _save_jsonl(ds.samples, OUTPUT_JSONL)
    
    print()
    print("=" * 80)
    print("âœ… é¢„è®¡ç®—å®Œæˆ!")
    print("=" * 80)
    print(f"è¾“å…¥è®°å½•: {len(ds.records)}")
    print(f"è¾“å‡ºæ ·æœ¬: {len(ds.samples)}")
    
    # ç»Ÿè®¡
    pos_count = sum(1 for s in ds.samples if s.get('start_positions', 0) != 0)
    neg_count = len(ds.samples) - pos_count
    
    print(f"æ­£æ ·æœ¬: {pos_count} ({pos_count/len(ds.samples)*100:.1f}%)")
    print(f"è´Ÿæ ·æœ¬: {neg_count} ({neg_count/len(ds.samples)*100:.1f}%)")
    
    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬çš„é—®é¢˜ç¤ºä¾‹
    print()
    print("ğŸ“ é—®é¢˜ç¤ºä¾‹ï¼ˆå‰5ä¸ªä¸åŒçš„é—®é¢˜ï¼‰ï¼š")
    seen_questions = set()
    count = 0
    for sample in ds.samples:
        if 'chunk_text' in sample:
            # ä» chunk_text ä¸­æå–é—®é¢˜ï¼ˆåœ¨ [CLS] å’Œ [SEP] ä¹‹é—´ï¼‰
            key = sample.get('question_key', '')
            if key and key not in seen_questions:
                seen_questions.add(key)
                # é€šè¿‡ alias æ˜ å°„è·å–é—®é¢˜
                rec_idx = sample.get('report_index', 0)
                if rec_idx < len(ds.records):
                    rec = ds.records[rec_idx]
                    question = ds._format_question(key, rec)
                    print(f"   {count+1}. {question}")
                    count += 1
                    if count >= 5:
                        break
    print()


if __name__ == "__main__":
    main()
