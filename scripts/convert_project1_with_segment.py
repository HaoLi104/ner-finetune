#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å°† Label Studio é¡¹ç›®å¯¼å‡ºè½¬æ¢ä¸º clean_ocr æ ¼å¼ï¼Œå¹¶æ™ºèƒ½åˆ†æ®µé•¿æ–‡æœ¬
"""
import sys
sys.path.append(".")

from pre_struct.ebqa.da_core.utils import convert_labelstudio_project_to_clean_records

# é…ç½®è·¯å¾„
IN_PATH = "data/project-1-at-2025-10-13-09-18-782e09b9.json"
OUT_PATH = "data/project-1.converted.json"

# è½¬æ¢å‚æ•°
MAX_REPORT_TOKENS = 512  # report è¶…è¿‡æ­¤é•¿åº¦æ—¶è‡ªåŠ¨åˆ†æ®µ
TOKENIZER_NAME = None    # å¯é€‰ï¼šæŒ‡å®š tokenizer è·¯å¾„ä»¥ç²¾ç¡®è®¡ç®— token

# å°è¯•ä»é…ç½®è·å– tokenizer
try:
    from model_path_conf import DEFAULT_TOKENIZER_PATH
    TOKENIZER_NAME = DEFAULT_TOKENIZER_PATH
    print(f"âœ“ ä½¿ç”¨ tokenizer: {TOKENIZER_NAME}")
except Exception:
    print("âš  æœªæ‰¾åˆ° tokenizerï¼Œä½¿ç”¨å­—ç¬¦æ•°ä¼°ç®— token")

print(f"\n=== Label Studio é¡¹ç›®è½¬æ¢ï¼ˆå«æ™ºèƒ½åˆ†æ®µï¼‰ ===")
print(f"è¾“å…¥: {IN_PATH}")
print(f"è¾“å‡º: {OUT_PATH}")
print(f"åˆ†æ®µé˜ˆå€¼: {MAX_REPORT_TOKENS} tokens")
print()

# æ‰§è¡Œè½¬æ¢
records = convert_labelstudio_project_to_clean_records(
    in_path=IN_PATH,
    out_path=OUT_PATH,
    max_report_tokens=MAX_REPORT_TOKENS,
    tokenizer_name=TOKENIZER_NAME,
)

print(f"âœ… è½¬æ¢å®Œæˆ")
print(f"   è®°å½•æ•°: {len(records)}")
print(f"   è¾“å‡º: {OUT_PATH}")

# ç»Ÿè®¡åˆ†æ®µæƒ…å†µ
segmented_count = sum(1 for r in records if "\n\n" in r.get("report", ""))
print(f"   å·²åˆ†æ®µ: {segmented_count} / {len(records)} ({segmented_count/len(records)*100:.1f}%)")

# æ˜¾ç¤ºç¤ºä¾‹
if records:
    print(f"\nğŸ“„ ç¤ºä¾‹è®°å½•:")
    sample = records[0]
    print(f"   æ ‡é¢˜: {sample.get('report_title', 'N/A')}")
    print(f"   å­—æ®µæ•°: {len([k for k in sample.keys() if k not in ('report', 'report_title', 'added_keys')])}")
    report = sample.get("report", "")
    print(f"   report é•¿åº¦: {len(report)} å­—ç¬¦")
    if "\n\n" in report:
        segments = report.split("\n\n")
        print(f"   å·²åˆ†æ®µ: {len(segments)} æ®µ")
    else:
        print(f"   æœªåˆ†æ®µï¼ˆé•¿åº¦é€‚ä¸­ï¼‰")

